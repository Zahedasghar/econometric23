<!DOCTYPE html>
<html lang="en"><head>
<script src="big_data_files/libs/clipboard/clipboard.min.js"></script>
<script src="big_data_files/libs/quarto-html/tabby.min.js"></script>
<script src="big_data_files/libs/quarto-html/popper.min.js"></script>
<script src="big_data_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="big_data_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="big_data_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="big_data_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="big_data_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.528">

  <meta name="author" content="Zahid Asghar">
  <title>Introduction to Big Data and Machine Learning in Economics</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="big_data_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="big_data_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="big_data_files/libs/revealjs/dist/theme/quarto.css">
  <link href="big_data_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="big_data_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="big_data_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="big_data_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="big_data_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="big_data_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <link href="big_data_files/libs/tile-view/tile-view.css" rel="stylesheet">
  <script src="big_data_files/libs/tile-view/tile-view.js"></script>
  <script src="big_data_files/libs/xaringanExtra_fit-screen/fit-screen.js"></script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title"><span class="flow">Introduction to Big Data and Machine Learning in Economics</span></h1>
  <p class="subtitle">Econometric Methods Lecture</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Zahid Asghar 
</div>
</div>
</div>

  <p class="date">08-12-2024</p>
</section>
<section id="in-this-lecture-you-will-learn" class="slide level2">
<h2>In this lecture, you will learn</h2>
<div>
<ul>
<li class="fragment">What is big data?</li>
<li class="fragment">The many prediction problems</li>
<li class="fragment">Ridge regression</li>
<li class="fragment">Lasso</li>
<li class="fragment">Principal components</li>
</ul>
</div>
</section>
<section id="shrinkage-estimators" class="slide level2 scrollable">
<h2>Shrinkage estimators</h2>
<div>
<ul>
<li class="fragment"><p><strong>Big data</strong>:Many observations and/or many predictors.</p></li>
<li class="fragment"><p>Includes datasets withmany categories.</p></li>
<li class="fragment"><p><strong>Challenges</strong>: Storing and accessing large data sets efficiently and developing fast algorithms for estimating models.</p></li>
<li class="fragment"><p><strong>OLS overfits in large studies</strong>: Use data on 3, 932 elementary schools in California to predict school-level test scores using data on school and community characteristics. Half of these observations are used to estimate prediction models, while the other half are reserved to test their performance. Large data sets have many predictors. The analysis starts with 817 predictors and is later expanded to 2065 predictors. With so many predictors, OLS overfits the data and makes poor out-of-sample predictions.</p></li>
<li class="fragment"><p><strong>Shrinkage estimators</strong>: Better out-of-sample predictors than OLS. Shrinkage estimators are biased and may not have a causal interpretation, but the variance of the shrinkage estimator is smaller, which improves out-of-sample prediction.</p></li>
</ul>
</div>
</section>
<section id="applications-of-big-data" class="slide level2 scrollable">
<h2>Applications of Big Data</h2>
<div>
<ol type="1">
<li class="fragment"><p><strong>Many predictors</strong>: Even if one starts with only a few dozen primitive predictors, including squares, cubes, and interactions very quickly expands the number of regressors into the hundreds or thousands.</p></li>
<li class="fragment"><p><strong>Categorization</strong>: Regression with a binary dependent variable and regression with many categories.</p></li>
<li class="fragment"><p><strong>Testing multiple hypotheses</strong>: The F-statistic of a joint hypothesis on a group of coefficients is not well suited to find out which of the treatments is effective. Special methods have been developed to test large numbers of individual hypotheses to determine which treatment effect is nonzero.</p></li>
<li class="fragment"><p><strong>Nonstandard data</strong>: Including text and images.</p></li>
<li class="fragment"><p><strong>Deep learning</strong>: Non-linear models are estimated (“trained”) using very many observations. Useful for pattern recognition, such as facial recognition; speech recognition; multi-language translation; detecting anomalies in medical scans; interpreting network data on social media; high-frequency trading in financial markets.</p></li>
</ol>
</div>
</section>
<section id="many-predictor-problem" class="slide level2 scrollable">
<h2>Many-Predictor Problem</h2>
<div>
<ul>
<li class="fragment"><p>The analysis of the district test score data reveals nonlinearities and interactions in the test score regressions. For example, there is a nonlinear relationship between test scores and the student-teacher ratio and this relationship differs depending on whether there are a large number of English learners in the district. These nonlinearities are handled by includ- ing third-degree polynomials of the student–teacher ratio and interaction terms.</p></li>
<li class="fragment"><p>If only the main variables are used, there are 38 regressors. Including interactions, squares, and cubes increases the number of predictors to 817.</p></li>
<li class="fragment"><p>We can also use a larger data set with 2, 065 predictors more than the 1, 966 observations in the estimation sample. While it does not violate the Gauss–Markov theorem, OLS can pro- duce poor predictions when the number of predictors is large relative to the sample size.</p></li>
</ul>
</div>
</section>
<section id="variables-in-the-817-predictor-school-test-score-data-set" class="slide level2 scrollable">
<h2>Variables in the 817-Predictor School Test Score Data Set</h2>
<p>Main variables (38)</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Fraction of students eligible for free or reduced-price lunch</p>
<p>Fraction of students eligible for free lunch Fraction of English learners</p>
<p>Teachers’ average years of experience</p>
<p>Instructional expenditures per student</p>
<p>Median income of the local population</p>
<p>Student–teacher ratio</p>
<p>Number of enrolled students</p>
<p>Fraction of English-language proficient students</p>
<p>Ethnic diversity index</p>
</div><div class="column" style="width:50%;">
<p>Ethnicity variables (8): fraction of students who are American Indian, Asian, Black, Filipino, Hispanic, Hawaiian, two or more, none reported Number of teachers</p>
<p>Fraction of first-year teachers</p>
<p>Fraction of second-year teachers</p>
<p>Part-time ratio (number of teachers divided by teacher full-time equivalents)</p>
<p>Per-student expenditure by category, district level (7)</p>
<p>Per-student expenditure by type, district level (5)</p>
<p>Per-student revenues by revenue source, district level (4)</p>
</div>
</div>
<ul>
<li>Squares of main variables (38)</li>
<li>Cubes of main variables (38)</li>
<li>All interactions of main variables 138 * 37&gt;2 = 7032 Total number of predictors = k = 38 + 38 + 38 + 703 = 817</li>
</ul>
</section>
<section id="mean-squared-prediction-error" class="slide level2 scrollable">
<h2>Mean Squared Prediction Error</h2>
<div>
<ul>
<li class="fragment"><p>Mean-Squared Prediction Error (MSPE): Expected value of the square of the prediction er- ror that arises when the model is used to make a prediction for an observation not in the data set.</p></li>
<li class="fragment"><p><span class="math inline">\(MSPE = E[Y^{OOS} − Y\hat(X^{OOS})^2]\)</span> where <span class="math inline">\(X^{OOS}\)</span> and <span class="math inline">\(Y^{OOS}\)</span> are out-of-sample observations on X and Y ; where <span class="math inline">\(Y\hat(X^{OOS})\)</span> is the predicted value of Y for some given value x.</p></li>
<li class="fragment"><p>Oracle prediction: <span class="math inline">\(E[Y^{OOS}|X^{OOS}]\)</span> The conditional mean minimizes the MSPE. It is not directly observable, so estimating it with the model coefficients of the prediction model introduces additional sources of error.</p></li>
<li class="fragment"><p>The Oracle prediction is the benchmark against which to judge all feasible predictions.</p></li>
</ul>
</div>
</section>
<section id="predictive-regression-model-with-standardized-regressors" class="slide level2 scrollable">
<h2>Predictive Regression Model with Standardized Regressors</h2>
<div>
<ul>
<li class="fragment"><p>Standardized regressors: Regressors are transformed to have mean 0 and variance 1. <span class="math inline">\(X_{ji} = \frac{X_{ji}^{orignal} - \mu_{X_j}^{original}}{\sigma_{X_j}^original}\)</span> where <span class="math inline">\(\mu_{X_j}^{original}\)</span> is the population mean of the original regressor.</p></li>
<li class="fragment"><p>Standardized regressand: The regressand is transformed to have mean 0.</p></li>
<li class="fragment"><p>Standardized predictive regression model: The intercept is excluded because all the variables have mean 0. <span class="math inline">\(Y_i = \beta_1X_{1i} + \beta_2X_{2i} + ... + \beta_kX_{ki} + u_i\)</span></p></li>
<li class="fragment"><p>The regression coefficients have the same units.</p></li>
<li class="fragment"><p>Interpretation: <span class="math inline">\(\beta_j\)</span> is the difference in the predicted value of Y associated with a one standard deviation difference in <span class="math inline">\(X_j\)</span>, holding all other regressors constant.</p></li>
</ul>
</div>
</section>
<section id="mspe-in-the-standardized-predictive-regression" class="slide level2 scrollable">
<h2>MSPE in the Standardized Predictive Regression</h2>
<p>Minimize the variance term, taking the bias as given - The standardized predictive regression model can be written as the sum of two components:</p>
<p><span class="math inline">\(MSPE = \sigma_u^2 + E[(\beta_1 - \beta_1^*)X_{1i} + (\beta_2 - \beta_2^*)X_{2i} + ... +\\ (\beta_k - \beta_k^*)X_{ki}]^2\)</span></p>
<div>
<ul>
<li class="fragment"><p>The mean-squared error is the sum of the bias and of the variance.</p></li>
<li class="fragment"><p>The first term <span class="math inline">\(\sigma_u^2\)</span> is the variance of the oracle prediction error: The prediction error made using the true (unknown) conditional mean.</p></li>
<li class="fragment"><p>The second term is the contribution to the prediction error arising from the estimated regression coefficients. This cost arises from estimating the coefficients instead of using true oracle prediction.</p></li>
<li class="fragment"><p>Objective: Minimize the variance term, taking the bias as given.</p></li>
<li class="fragment"><p>Prediction for an out-of-sample observation: Because the regressors are standardized and the dependent variable is demeaned, the out-of-sample observation on the predictors must be standardized using the in-sample mean and standard deviation, and the in-sample mean of the dependent variable must be added back into the prediction.</p></li>
</ul>
</div>
</section>
<section id="ols-is-best-linear-unbiased-predictor-blup" class="slide level2 scrollable">
<h2>OLS is Best Linear Unbiased Predictor (BLUP)</h2>
<div>
<ul>
<li class="fragment"><p>In the special case of homoskedastic regression errors, the MSPE of OLS is given by <span class="math inline">\(MSPE ≈ \sigma_u^2(1 + \frac{k}{n})\)</span> The approximation more accurate for large n and small k/n.</p></li>
<li class="fragment"><p>The cost of using OLS, as measured by the MSPE, depends on the ratio of the number of regressors to the sample size.</p></li>
<li class="fragment"><p>In the school test score application with 38 regressors, using OLS has a loss of only 2% relative to the Oracle prediction. But with 817 regressors, the loss increases to 40%. <span class="math inline">\(\frac{k}{n} = \frac{38}{1, 966} ≈ 0.02\)</span> <span class="math inline">\(\frac{k}{n} = \frac{817}{1, 966} ≈ 0.40\)</span></p></li>
<li class="fragment"><p>Because OLS is unbiased, the loss is entirely due to the variance term. Under Gauss-Markov, this is the smallest loss in the class of linear, unbiased estimators.</p></li>
<li class="fragment"><p>The loss can be reduced … using biased estimators!</p></li>
</ul>
</div>
</section>
<section id="the-principle-of-shrinkage" class="slide level2 scrollable">
<h2>The Principle of Shrinkage</h2>
<div>
<ul>
<li class="fragment"><p>Shrinkage estimator: Introduces bias by “shrinking” the OLS estimator toward a specific number and thereby reducing the variance of the estimator.</p></li>
<li class="fragment"><p>Because the mean squared error is the sum of the variance and the squared bias, if the esti- mator variance is reduced by enough, then the decrease in the variance can more than com- pensate for the increase in the squared bias.</p></li>
<li class="fragment"><p>James-Stein estimator: When the regressors are uncorrelated, the James-Stein estimator can be written <span class="math inline">\(\tilde\beta^{JS}=c\hat{\beta}\)</span>, where <span class="math inline">\(\hat{\beta}\)</span> is the OLS estimator and c is a factor that is less than 1 and depends on the data. Since c1, the JS estimator shrinks the OLS estimator toward 0 and thus is biased toward 0.</p></li>
<li class="fragment"><p>James and Stein (1961): If the errors are normally distributed and <span class="math inline">\(k≥ 3\)</span>, their estimator has a lower mean squared error than the OLS estimator, regardless of the true value of β.</p></li>
<li class="fragment"><p>James-Stein leads to the family of shrinkage estimators, which includes ridge regression and the Lasso estimator.</p></li>
</ul>
</div>
</section>
<section id="estimation-by-split-sample" class="slide level2">
<h2>Estimation by Split Sample</h2>
<p>Split sample:</p>
<ul>
<li>Estimate the MSPE by dividing the data set into two parts</li>
<li>an “estimation” subsample.</li>
<li>a “test” subsample used to simulate out-of- sample prediction. <span class="math inline">\(MSPE_{split-sample} = \frac{1}{n_{test-subsample}}\sum_{n_{test-subsample}}(Y_i - \hat{Y}_i)^2\)</span></li>
</ul>
</section>
<section id="estimation-by-m-fold-cross-validation" class="slide level2 scrollable">
<h2>Estimation by m-fold Cross Validation</h2>
<p>m-fold cross validation: 1. Estimate the MSPE by dividing the data set into two parts: an “estimation” subsample and a “test” subsample used to simulate out-of-sample prediction.<br>
2. Use the combined sub-samples 2, 3, … , m to compute <span class="math inline">\(\tilde\beta\)</span>, an estimate of β.</p>
<ol start="3" type="1">
<li><p>Use <span class="math inline">\(\tilde\beta\)</span> to compute predicted values <span class="math inline">\(\hat{Y}\)</span> and prediction errors <span class="math inline">\(Y-\hat{Y}\)</span> for sub-sample 1.</p></li>
<li><p>Using sub-sample 1 as the test sample, estimate the MSPE with the predicted values in sub-sample 1.</p></li>
<li><p>Repeat steps 2-4 leaving out sub-sample 2, then 3, …, then m.</p></li>
<li><p>The m-fold cross-validation estimator of the MSPE is estimated by averaging these m sub-sample estimates of the MSPE.</p></li>
</ol>
<p><span class="math inline">\(MSPE_{m-fold cross-validation} = \frac{1}{m}\sum_{i}\widehat{MSPE_i}\)</span> where <span class="math inline">\({n_i}\)</span> is the number of observations in sub-sample i.</p>
</section>
<section id="estimation-by-m-fold-cross-validation-1" class="slide level2 scrollable">
<h2>Estimation by m-fold Cross Validation</h2>
<p>The tradeoff</p>
<ul>
<li><p>Choosing the value of m involves a tradeoff between efficiency of the estimators and computational requirements.</p></li>
<li><p>More observations: A larger value of m produces more efficient estimators of β, because more observations are used to estimate β. leave-one-out cross-validation estimator: Set m = n − 1. This maximizes the number of observations used.</p></li>
<li><p>More computations:A larger value ofmimplies that β must be estimatedmtimes. The leave- one-out cross validation may demand too much computational power.</p></li>
<li><p>School test score application: A compromise value m = 10 is selected, meaning that each sub-sample estimator of β uses 90% of the sample.</p></li>
</ul>
</section>
<section id="ridge-regression" class="slide level2 scrollable">
<h2>Ridge Regression</h2>
<p>Penalized Sum of Squared Residuals - Penalty: To shrink the estimated coefficients toward 0, penalize large values of the estimate. - Ridge regression estimator: Minimizes the penalized sum of squares - the sum of squared residuals plus a penalty factor that increases with the sum of the squared coefficients: <span class="math inline">\(S^{Ridge}(b;\lambda_{Ridge}) = \sum_{i=1}^n(Y_i - b_1X_{1i} - ... - b_kX_{ki})^2 + \lambda_{Ridge}\sum_{j=1}^kb_j^2\)</span></p>
<ul>
<li><p>Ridge shrinkage parameter: λRidge ≥ 0.</p></li>
<li><p>First term: Sum of squared residuals for candidate estimator b.</p></li>
<li><p>Second term: Penalizes the estimator for choosing a large estimate of the coefficient.</p></li>
<li><p>In the special case that the regressors are uncorrelated, the ridge regression estimator is: <span class="math inline">\(\tilde\beta^{Ridge} = [\frac{1}{1 + \frac{\lambda_{Ridge}}{\sum_{j=1}X_{ji}^2}}]\hat\beta\)</span></p></li>
</ul>
<p>where <span class="math inline">\(\hat\beta_j\)</span> is the OLS estimator of <span class="math inline">\(β_j\)</span>.</p>
<div class="fragment">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/Ridge.png"></p>
<figcaption>Ridge Regression</figcaption>
</figure>
</div>
</div>
</section>
<section id="ridge-regression-shrinkage" class="slide level2">
<h2>Ridge Regression: Shrinkage</h2>
<p>Choosing the Shrinkage Parameter</p>
<ul>
<li><p>Choose λRidge to minimize the estimated MSPE, using the m-fold cross-validation estimator of the MSPE.</p></li>
<li><p>Suppose you have two candidate values 0.1 and 0.2. Let β ̃be the Ridge estimator for a given value ofλRidge. Compute the predictions in the test sample and corresponding <span class="math inline">\(\widehat{MSPE}\)</span>. Compare the values of MSPE obtained for λRidge = 0.1 and λRidge = 0.2. Select the smaller of the two values.</p></li>
<li><p>Typically the Ridge estimator that minimizes the m-fold cross-validation MSPE differs from the OLS estimator.</p></li>
</ul>
</section>
<section id="ridge-regression-for-school-test-scores" class="slide level2 scrollable">
<h2>Ridge Regression for School Test Scores</h2>
<ul>
<li><p>Fit a predictive model for school test scores using 817 predictors with 1, 966 observations.</p></li>
<li><p>The square root of the MSPE estimates the magnitude of a typical out-of-sample prediction error.</p></li>
<li><p>The choice of m = 10 represents a practical balance between the desire to use as many observations as possible to estimate the parameters and the computational burden of re-peating that estimation m times for each value of λRidge.</p></li>
<li><p>The MSPE is minimized for <span class="math inline">\(\hat\lambda_{Ridge} = 2.233\)</span>.</p></li>
<li><p>The square-root of MSPE evaluated at <span class="math inline">\(\lambda_{Ridge}=\hat\lambda_{Ridge}\)</span> is about 39.5.</p></li>
<li><p>The square-root of MSPE evaluated at λRidge = 0 — the OLS estimator — is about 78.2.</p></li>
<li><p>Because <span class="math inline">\(\hat\lambda_{Ridge}\)</span> minimizes the cross-validated MSPE, the cross-validated MSPE evaluated at <span class="math inline">\(\hat\lambda_{Ridge}\)</span> is a biased estimator of the MSPE. We therefore use the remaining 1966 observations to obtain an unbiased estimator of the MSPE for ridge regression using <span class="math inline">\(\hat\lambda_{Ridge}\)</span>.</p></li>
</ul>
</section>
<section id="section" class="slide level2">
<h2></h2>

<img data-src="images/ridge1.png" class="r-stretch"></section>
<section id="lasso-regression" class="slide level2 scrollable">
<h2>Lasso Regression</h2>
<p>Penalized Sum of Squared Residuals - The Lasso estimator minimizes a penalized sum of squares, where the penalty increases with the sum of the absolute values of the coefficients:</p>
<p><span class="math inline">\(S^{Lasso}(b; \lambda_{Lasso}) = \sum_{i=1}^{n}(Y_i - b_1X_{1i} - ... - b_kX_{ki})^2 + \lambda_{Lasso}\sum_{j=1}^{k}|b_j|\)</span></p>
<p>where <span class="math inline">\(\lambda_{Lasso}\)</span> is the Lasso shrinkage parameter.</p>
<ul>
<li>Sparse model:A regression model in which the coefficients are nonzero for only a small frac- tion of the predictors. In sparse models, predictions can be improved by estimating many of the coefficients to be exactly 0. The Lasso regression sets many of the estimated coefficients exactly to 0. The regressors it keeps are subject to less shrinkage than with Ridge regression.</li>
<li>Ridge regression penalty increases with the square <span class="math inline">\(b^2\)</span>.</li>
<li>Lasso regression penalty increases with the absolute value |b|.</li>
<li>For large values of b, the ridge penalty exceeds the Lasso penalty. If the OLS estimate is large, the Lasso shrinks it less than Ridge; if the OLS estimate is small, the Lasso shrinks it more.</li>
</ul>
<div class="fragment">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/lasso.png"></p>
<figcaption>Lasso Regression</figcaption>
</figure>
</div>
</div>
</section>
<section id="lasso-regression-shrinkage" class="slide level2">
<h2>Lasso Regression: Shrinkage</h2>
<p>Choosing the Shrinkage Parameter</p>
<ul>
<li><p>Unlike OLS and ridge regression, there is no simple expression for the Lasso estimator when <span class="math inline">\(k &gt; 1\)</span>, so the Lasso minimization problem must be done using specialized algorithms. Recent advances in machine learning have made it easier to compute Lasso problems.</p></li>
<li><p>With Ridge and Lasso regressions, the estimated coefficients depend on the specific choice of the linear combination of regressors used. Selecting regressors requires more care because predictions are more sensitive to the choice of regressors.</p></li>
<li><p>Using the California Schools data on test scores yields: <span class="math inline">\(MSPE_{OLS} = 78.2\)</span> , <span class="math inline">\(\hat\lambda_{Lasso} = 4,527\)</span> , <span class="math inline">\(MSPE_{Lasso} = 39.7\)</span>, <span class="math inline">\(\hat\lambda_{Ridge} = 2, 233\)</span>, <span class="math inline">\(MSPE_{Ridge} = 39.5\)</span>.</p></li>
</ul>
</section>
<section id="section-1" class="slide level2">
<h2></h2>

<img data-src="images/lasso1.png" class="r-stretch"></section>
<section id="principal-components" class="slide level2 scrollable">
<h2>Principal Components</h2>
<ul>
<li><p>If two regressors are perfectly collinear, one of them must be dropped — to avoid falling into the dummy variable trap.</p></li>
<li><p>This suggests dropping a variable if it is highly correlated (even imperfectly) with the other regressors.</p></li>
<li><p>Principal components analysis implements this strategy — to avoid falling into the “too many variables trap”. Linear combinations of variables selected so that the principal components are mutually uncorrelated and keep as much information as possible.</p></li>
<li><p><strong>Principal Components with 2 Variables</strong>:</p></li>
<li><p>The linear combination weights for the first principal component are chosen to maximize its variance — to capture as much of the variation as possible.</p></li>
<li><p>The linear combination weights for the second principal component are chosen to be uncorrelated with the first principal component and to capture as much of the variance as possible after controlling for the first principal component.</p></li>
<li><p>The linear combination weights for the third principal component are chosen to be uncorrelated with the first two components and, again, to capture as much of the variance as possible, after controlling for the first two components.</p></li>
<li><p>And again for the fourth, fifth, …, nth components.</p></li>
</ul>
</section>
<section id="principal-components-two-variables" class="slide level2 scrollable">
<h2>Principal Components: Two Variables</h2>
<ul>
<li><p>Let <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> be standard normal random variables with population correlation <span class="math inline">\(\rho=0.7\)</span></p></li>
<li><p>The first principal component is the weighted average, <span class="math inline">\(PC_1 = w_1X_1 + w_2X_2\)</span>, with the maximum variance, where w1 and w2 are the principal component weights.</p></li>
<li><p>The second principal component is chosen to be uncorrelated with the first. This minimizes the spread of the variables.</p></li>
<li><p>When there are only two variables, the first principal component maximizes the variance of the linear combination, while the second principal component minimizes the variance of the linear combination.</p></li>
<li><p>Together the two principal components explain all of the variance of X. The fraction of the total variance explained by the principal components are: <span class="math inline">\(\frac{var(PC_1)}{var(X_1) + var(X_2)} = 0.85\)</span> and <span class="math inline">\(\frac{var(PC_2)}{var(X_1) + var(X_2)} = 0.15\)</span>.</p></li>
<li><p>The variances are <span class="math inline">\(var(PC_1) = 1+ |\rho|\)</span> and <span class="math inline">\(var(PC_2) = 1- |\rho|\)</span>, where <span class="math inline">\(cov(X_1, X_2) = \rho\)</span>.</p></li>
</ul>
</section>
<section id="first-and-second-principal-components" class="slide level2 scrollable">
<h2>First and second principal components</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="images/pc.png" width="771"></p>
</div><div class="column" style="width:50%;">
<p>Two standard normal random variables, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> , with population correlation 0.7. The first principal component (PC1) maximizes the variance of the linear combination of these variables, which is done by adding X1 and X2. The second principal component (PC2) is uncorrelated with the first and is obtained by subtracting the two variables. The principal component weights are normalized so that the sum of squared weights adds to 1. The spread of the variables is greatest in the direction of the <span class="math inline">\(45^o\)</span> line. Along the <span class="math inline">\(45^o\)</span> line, the weights are equal, so w1 = w2 = 1/√2 and <span class="math inline">\(PC_1 = (X_1 + X_2)/\sqrt{2}\)</span> and <span class="math inline">\(PC_2 = (X_1 - X_2)/\sqrt{2}\)</span>.</p>
<ul>
<li>The first component explains (1+ρ)/2 = 85%</li>
<li>The second component explains 15%.</li>
</ul>
</div>
</div>
</section>
<section id="principal-components-1" class="slide level2 scrollable">
<h2>Principal Components</h2>
<p>The Principal Components of the k variables <span class="math inline">\(X_1,\dots,X_k\)</span> are the linear combinations of the Xs that have the following properties: 1. The squared weights of the linear combinations sum to 1. 2. The first principal component maximizes the variance of its linear combination. 3. The second principal component maximizes the variance of its linear combination, subject to its being uncorrelated with the first principal component. 4. The jth principal component maximizes the variance of its linear combination, subject to its being uncorrelated with the first <span class="math inline">\(j-1\)</span> principal components.</p>
<ul>
<li>The number of principal components is the minimum of n and k.</li>
<li>The sum of the sample variances of the principal components equals the sum of the sample variances of the Xs:</li>
</ul>
<p><span class="math inline">\(\sum_{j=1}^{min(n,k)} var(PC_j) = \sum_{j=1}^k var(X_j)\)</span></p>
<ul>
<li>The ratio <span class="math inline">\(var(PC_j)/\sum_{j=1}^k var(X_j)\)</span> is the fraction of the total variance of the Xs explained by the jth principal component. This measure is like an <span class="math inline">\(R^2\)</span> for the total variance of the Xs.</li>
</ul>
</section>
<section id="square-root-of-the-mspe-for-principal-components-prediction" class="slide level2">
<h2>Square root of the MSPE for principal components prediction</h2>

<img data-src="images/pc2.png" class="r-stretch quarto-figure-center"><p class="caption">Square root of the MSPE for principal components prediction</p></section>
<section id="principal-components-application-to-school-test-scores" class="slide level2">
<h2>Principal Components Application to School Test Scores:</h2>
<ul>
<li>Initially, increasing the number of principal components used as predictors results in a sharp decline in the MSPE.</li>
<li>After p = 5 principal components, the improvement slows down; and after p = 23 principal components, the MSPE is essentially flat in the number of predictors.</li>
<li>TheMSPE isminimized at 46 predictors. The cross-validation estimate is ̂<span class="math inline">\(\hat{p}=46\)</span>, with <span class="math inline">\(MSPE=39.7\)</span>, similar to Lasso and Ridge.</li>
</ul>
</section>
<section id="prediction-procedure" class="slide level2">
<h2>Prediction Procedure</h2>
<ul>
<li><p>Do the many-predictor methods improve upon test score predictions made using OLS with a small data set and, if so, how do the many-predictor methods compare?</p></li>
<li><p>Predict school test scores using small (k = 4), large (k = 817), and very large (k = 2065) data sets, using OLS, Ridge, Lasso, and PC.</p></li>
<li><p>1966 observations in the estimation sample, we estimate the shrinkage parameter by 10-fold cross validation. Using this estimated shrinkage parameter, the regression coefficients are re-estimated using all 1966 observations in the estimation sample. Those estimated coefficients are then used to predict the out-of-sample values for all the observations in the reserved sample for other half.</p></li>
</ul>
</section>
<section id="prediction-procedure-1" class="slide level2 scrollable">
<h2>Prediction Procedure</h2>
<p>Standout Features:</p>
<ol type="1">
<li><p>The MSPE of OLS is much less using the small data set than using the large data set.</p></li>
<li><p>There are substantial gains from increasing the number of predictors from 4 to 817, with the square root of the MSPE falling by roughly one-fourth, but not much beyond that.</p></li>
<li><p>The in-sample estimates of MSPE (the 10-fold cross-validation estimates) are similar to the out-of-sample estimates. This is mainly because the coefficients used for the out-of-sample estimate of the MSPE are estimated using all 1966 observations in the estimation sample.</p></li>
<li><p>The MSPE in the reserved test sample is generally similar for all the many-predictor methods. The lowest out-of- sample MSPE is obtained using Ridge in the large data set.</p></li>
<li><p>For the large data set, the many-predictor methods succeed where OLS fails: The many-predictor methods allow the coefficient estimates to be biased in a way that reduces their variance by enough to compensate for the increased bias.</p></li>
</ol>
</section>
<section id="section-2" class="slide level2">
<h2></h2>

<img data-src="images/3sets.png" width="686" class="r-stretch"></section>
<section id="section-3" class="slide level2">
<h2></h2>

<img data-src="images/table.png" class="r-stretch quarto-figure-center"><p class="caption">OOS Performance of Predictive Models for STR</p></section>
<section id="section-4" class="slide level2">
<h2></h2>

<img data-src="images/table2.png" class="r-stretch"></section>
<section id="section-5" class="slide level2">
<h2></h2>

<img data-src="images/graph.png" class="r-stretch"></section>
<section id="summary" class="slide level2 scrollable">
<h2>Summary</h2>
<p>The goal of prediction is to make accurate predictions for out-of-sample observations. The coefficients in prediction models do not have a causal interpretation. OLS works poorly for prediction when the number of regressors is large relative to the sample size. The shortcomings of OLS can be overcome by using prediction methods that have lower variance at the cost of introducing estimator bias. These many-predictor methods can produce predictions with substantially better predictive performance than OLS, as measured by the MSPE. Ridge regression and the Lasso are shrinkage estimators that minimize a penalized sum of squared residuals. The penalty introduces a cost to estimating large values of the regression coefficient. The weight on the penalty (the shrinkage parameter) can be estimated by minimizing the m-fold cross-validation estimator of the MSPE. The principal components of a set of correlated variables capture most of the variation in those variables in a reduced number of linear combinations. Those principal components can be used in a predictive regression, and the number of principal components included can be estimated by minimizing the m-fold cross-validation MSPE.</p>
<ol type="1">
<li>The goal of prediction is to make accurate predictions for out-of-sample observations. The coefficients in prediction models do not have a causal interpretation.</li>
<li>OLS works poorly for prediction when the number of regressors is large relative to the sample size.</li>
<li>The shortcomings of OLS can be overcome by using prediction methods that have lower variance at the cost of introducing estimator bias. These many-predictor methods can produce predictions with substantially better predictive performance than OLS, as measured by the MSPE.</li>
<li>Ridge regression and the Lasso are shrinkage estimators that minimize a penalized sum of squared residuals. The penalty introduces a cost to estimating large values of the regression coefficient. The weight on the penalty (the shrinkage parameter) can be estimated by minimizing the m-fold cross-validation estimator of the MSPE.</li>
<li>The principal components of a set of correlated variables capture most of the variation in those variables in a reduced number of linear combinations. Those principal components can be used in a predictive regression, and the number of principal components included can be estimated by minimizing the m-fold cross-validation MSPE.</li>
</ol>
</section>
<section id="problems-and-applications" class="slide level2 scrollable">
<h2>Problems and Applications</h2>
<p>Stock &amp; Watson, Introduction (4th), Chapter 14, Exercise 1. A researcher is interested in predicting average test scores for elementary schools in Arizona. She collects data on three variables from 200 randomly chosen Arizona elementary schools:</p>
<p>average test scores (TestScore) on a standardized test, the fraction of students who qualify for reduced-priced meals (RPM), and the average years of teaching experience for the school’s teachers (TExp). The table below shows the sample means and standard deviations from her sample.</p>
<table>
<thead>
<tr class="header">
<th>Variable</th>
<th>Sample Mean</th>
<th>Sample Standard Deviation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>TestScore</td>
<td>750.1</td>
<td>65.9</td>
</tr>
<tr class="even">
<td>RPM</td>
<td>0.60</td>
<td>0.28</td>
</tr>
<tr class="odd">
<td>TExp</td>
<td>13.2</td>
<td>3.8</td>
</tr>
</tbody>
</table>
<p>After standardizing RPM and Texp and substracting the sample mean from TestScore, she estimates the following regression:</p>
<p><span class="math inline">\(TestScore = −48.7RPM + 8.7TExp + 775.3, SER = 44.0\)</span></p>
<ol type="1">
<li>You are interested in using the estimated regression to predict average test scores for an out-of-sample school with RPM = 0.52 and TExp = 11.1.</li>
</ol>
<!-- -->
<ol type="a">
<li><p>Compute the transformed (standardized) values of RPM and TExp for this school; that is, compute the <span class="math inline">\(\hat{X}^{oos}\)</span> values from the <span class="math inline">\({X}^{\ast{oos}}\)</span> values.</p></li>
<li><p>Compute the predicted value of average test scores for this school.</p></li>
</ol>
<!-- -->
<ol start="2" type="1">
<li><p>The actual average test score for the school is 775.3. Compute the error for your prediction.</p></li>
<li><p>The regression shown above was estimated using the standardized regressors and the demeaned value of TestScore. Suppose the refrgression had been estimated using the raw data for TestScore, RPM, and TExp. Calculate the values of the regression intercept and slope coefficients for this regression.</p></li>
<li><p>Use the regression coefficients that you computed in 3 to predict average test scores for an out-of-sample school with RPM = 0.52 and TExp = 11.1. Verify that the prediction is identical to the prediction you computed in 2.</p></li>
</ol>

</section>
    <div class="quarto-auto-generated-content">
<div class="footer footer-default">

</div>
</div></div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="big_data_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="big_data_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="big_data_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="big_data_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="big_data_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="big_data_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="big_data_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="big_data_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="big_data_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="big_data_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="big_data_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>